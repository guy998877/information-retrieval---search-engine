{"cells":[{"cell_type":"markdown","id":"51cf86c5","metadata":{"id":"01ec9fd3"},"source":["# Imports & Setup"]},{"cell_type":"code","execution_count":1,"id":"5ac36d3a","metadata":{"id":"c0ccf76b","nbgrader":{"grade":false,"grade_id":"cell-Worker_Count","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"cf88b954-f39a-412a-d87e-660833e735b6"},"outputs":[{"name":"stdout","output_type":"stream","text":["NAME          PLATFORM  PRIMARY_WORKER_COUNT  SECONDARY_WORKER_COUNT  STATUS   ZONE           SCHEDULED_DELETE\r\n","cluster-ec23  GCE       4                                             RUNNING  us-central1-a\r\n"]}],"source":["!gcloud dataproc clusters list --region us-central1"]},{"cell_type":"code","execution_count":2,"id":"bf199e6a","metadata":{"id":"32b3ec57","nbgrader":{"grade":false,"grade_id":"cell-Setup","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"fc0e315d-21e9-411d-d69c-5b97e4e5d629"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n","\u001B[0m\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n","\u001B[0m"]}],"source":["!pip install -q google-cloud-storage==1.43.0\n","!pip install -q graphframes"]},{"cell_type":"code","execution_count":3,"id":"d8f56ecd","metadata":{"id":"5609143b","nbgrader":{"grade":false,"grade_id":"cell-Imports","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"a24aa24b-aa75-4823-83ca-1d7deef0f0de"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}],"source":["import pyspark\n","import sys\n","import itertools\n","from itertools import islice, count, groupby\n","from math import log\n","from operator import itemgetter\n","from collections import Counter, OrderedDict, defaultdict\n","import pandas as pd\n","import os\n","import re\n","import math\n","import numpy as np\n","import nltk\n","from nltk.stem.porter import *\n","from nltk.stem import PorterStemmer\n","from nltk.corpus import stopwords\n","from time import time\n","from heapq import heappop, heappush, heapify\n","from threading import Thread\n","from flask import Flask, request, jsonify\n","import pickle\n","import requests\n","from pathlib import Path\n","from google.cloud import storage\n","nltk.download('stopwords')\n","\n","import hashlib\n","def _hash(s):\n","    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()"]},{"cell_type":"code","execution_count":4,"id":"38a897f2","metadata":{"id":"b10cc999","nbgrader":{"grade":false,"grade_id":"cell-jar","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"8f93a7ec-71e0-49c1-fc81-9af385849a90"},"outputs":[{"name":"stdout","output_type":"stream","text":["-rw-r--r-- 1 root root 247882 Mar  9 09:17 /usr/lib/spark/jars/graphframes-0.8.2-spark3.1-s_2.12.jar\r\n"]}],"source":["# initialization cluster\n","!ls -l /usr/lib/spark/jars/graph*"]},{"cell_type":"code","execution_count":5,"id":"47900073","metadata":{"id":"d3f86f11","nbgrader":{"grade":false,"grade_id":"cell-pyspark-import","locked":true,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["from pyspark.sql import *\n","from pyspark.sql.functions import *\n","from pyspark import SparkContext, SparkConf, SparkFiles\n","from pyspark.sql import SQLContext\n","from pyspark.sql import SparkSession\n","from graphframes import *"]},{"cell_type":"code","execution_count":6,"id":"72bed56b","metadata":{"id":"5be6dc2a","nbgrader":{"grade":false,"grade_id":"cell-spark-version","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"07b4e22b-a252-42fb-fe46-d9050e4e7ca8","scrolled":true},"outputs":[{"data":{"text/html":["\n","            <div>\n","                <p><b>SparkSession - hive</b></p>\n","                \n","        <div>\n","            <p><b>SparkContext</b></p>\n","\n","            <p><a href=\"http://cluster-ec23-m.c.project3-map-reduce.internal:36017\">Spark UI</a></p>\n","\n","            <dl>\n","              <dt>Version</dt>\n","                <dd><code>v3.3.2</code></dd>\n","              <dt>Master</dt>\n","                <dd><code>yarn</code></dd>\n","              <dt>AppName</dt>\n","                <dd><code>PySparkShell</code></dd>\n","            </dl>\n","        </div>\n","        \n","            </div>\n","        "],"text/plain":["<pyspark.sql.session.SparkSession at 0x7f3c0d186530>"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["spark"]},{"cell_type":"code","execution_count":7,"id":"980e62a5","metadata":{"id":"7adc1bf5","nbgrader":{"grade":false,"grade_id":"cell-bucket_name","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[],"source":["# access to the bucket\n","bucket_name = 'yuval_206542839' \n","full_path = f\"gs://{bucket_name}/\"\n","paths=[]\n","\n","client = storage.Client()\n","blobs = client.list_blobs(bucket_name)\n","for b in blobs:\n","    if b.name != 'graphframes.sh' and b.name.endswith(\".parquet\"):\n","        paths.append(full_path+b.name)"]},{"cell_type":"markdown","id":"cac891c2","metadata":{"id":"13ZX4ervQkku"},"source":["***GCP setup is complete!***"]},{"cell_type":"markdown","id":"582c3f5e","metadata":{"id":"c0b0f215"},"source":["# Building an inverted index"]},{"cell_type":"code","execution_count":8,"id":"e4c523e7","metadata":{"id":"b1af29c9","scrolled":false},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["# entire corpus \n","parquetFile = spark.read.parquet(*paths)\n","doc_text_pairs = parquetFile.select(\"text\",\"id\").rdd"]},{"cell_type":"code","execution_count":9,"id":"82881fbf","metadata":{"id":"d89a7a9a"},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["# Count number of wiki pages\n","num_documents = parquetFile.count()"]},{"cell_type":"code","execution_count":10,"id":"121fe102","metadata":{"id":"04371c88","outputId":"327fe81b-80f4-4b3a-8894-e74720d92e35"},"outputs":[{"name":"stdout","output_type":"stream","text":["inverted_index_gcp.py\r\n"]}],"source":["# check if inverted_index_gcp.py is uploaded to the home dir\n","%cd -q /home/dataproc\n","!ls inverted_index_gcp.py"]},{"cell_type":"code","execution_count":11,"id":"57c101a8","metadata":{"id":"2d3285d8","scrolled":true},"outputs":[],"source":["# adding python module to the cluster\n","sc.addFile(\"/home/dataproc/inverted_index_gcp.py\")\n","sys.path.insert(0,SparkFiles.getRootDirectory())"]},{"cell_type":"code","execution_count":12,"id":"c259c402","metadata":{"id":"2477a5b9"},"outputs":[],"source":["from inverted_index_gcp import *\n","from inverted_index_gcp import InvertedIndex"]},{"cell_type":"markdown","id":"55e8a31b","metadata":{},"source":["***preproccesing***\n","remove stop words (enslish & corpus)\n","filter by ReGex\n","PorterStemmer\n","create functions that will calculate tf, idf, document lengths\n","and functions that will write posting list to bucket\n","\n","\n"]},{"cell_type":"code","execution_count":13,"id":"f3ad8fea","metadata":{"id":"a4b6ee29","nbgrader":{"grade":false,"grade_id":"cell-token2bucket","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[],"source":["english_stopwords = frozenset(stopwords.words('english'))\n","corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\",\n","                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\",\n","                    \"part\", \"thumb\", \"including\", \"second\", \"following\",\n","                    \"many\", \"however\", \"would\", \"became\"]\n","\n","all_stopwords = english_stopwords.union(corpus_stopwords)\n","RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n","\n","NUM_BUCKETS = 124\n","def token2bucket_id(token):\n","  return int(_hash(token),16) % NUM_BUCKETS\n","\n","porter = PorterStemmer()\n","\n","def word_count(text, id):\n","  ''' Count the frequency of each word in `text` (tf) that is not included in\n","  `all_stopwords` and return entries that will go into our posting lists.\n","  Parameters:\n","  -----------\n","    text: str\n","      Text of one document\n","    id: int\n","      Document id\n","  Returns:\n","  --------\n","    List of tuples\n","      A list of (token, (doc_id, tf)) pairs\n","      for example: [(\"Anarchism\", (12, 5)), ...]\n","  '''\n","  tokens = [token.group() for token in RE_WORD.finditer(text.lower()) if token.group() not in all_stopwords]\n","    \n","  # Apply stemming to the tokens\n","  stemmed_tokens = [porter.stem(token) for token in tokens]\n","    \n","  # Count the frequency of each word using `Counter` from the collections module\n","  freq = Counter(stemmed_tokens)\n","\n","  # Convert the `freq` dictionary to a list of tuples as required\n","  result = [(token, (id, count)) for token, count in freq.items()]\n","\n","  return result\n","\n","def length_doc(text, id):\n","  ''' Calculate the length of a document.\n","  Parameters:\n","  -----------\n","    text: str\n","      Text of one document\n","    id: int\n","      Document id\n","  Returns:\n","  --------\n","    tuple (doc_id, doc_length) \n","  '''\n","  tokens = [token.group() for token in RE_WORD.finditer(text.lower()) if token.group() not in all_stopwords]\n","    \n","  # Apply stemming to the tokens\n","  stemmed_tokens = [porter.stem(token) for token in tokens]\n","\n","  return (id,len(stemmed_tokens))\n","\n","\n","\n","def reduce_word_counts(unsorted_pl):\n","  ''' Returns a sorted posting list by wiki_id.\n","  Parameters:\n","  -----------\n","    unsorted_pl: list of tuples\n","      A list of (wiki_id, tf) tuples\n","  Returns:\n","  --------\n","    list of tuples\n","      A sorted posting list.\n","  '''\n","\n","  # Sort the unsorted posting list by wiki_id\n","  sorted_pl = sorted(unsorted_pl, key=lambda x: x[0])\n","  return sorted_pl\n","\n","\n","\n","def calculate_df(postings):\n","  ''' Takes a posting list RDD and calculate the df for each token.\n","  Parameters:\n","  -----------\n","    postings: RDD\n","      An RDD where each element is a (token, posting_list) pair.\n","  Returns:\n","  --------\n","    RDD\n","      An RDD where each element is a (token, df) pair.\n","  '''\n","  return postings.map(lambda x: (x[0], len(x[1])))\n","\n","\n","\n","def partition_postings_and_write(postings, bucket_name):\n","  ''' A function that partitions the posting lists into buckets, writes out\n","  all posting lists in a bucket to disk, and returns the posting locations for\n","  each bucket. Partitioning is done using the `token2bucket_id` function.\n","\n","  Parameters:\n","  -----------\n","    postings: RDD\n","      An RDD where each item is a (w, posting_list) pair.\n","    bucket_name: str\n","      The name of the bucket where the posting lists will be stored.\n","\n","  Returns:\n","  --------\n","    RDD\n","      An RDD where each item is a posting locations dictionary for a bucket.\n","  '''\n","  # Map postings to buckets based on token\n","  postings_bucketed = postings.map(lambda posting: (token2bucket_id(posting[0]), posting))\n","\n","  # Group postings by bucket ID\n","  postings_grouped = postings_bucketed.groupByKey().mapValues(list)\n","\n","  # Write postings to disk and collect location information\n","  def write_postings_to_disk(postings_list):\n","      # Pass `bucket_name` to `write_a_posting_list` method\n","      return InvertedIndex.write_a_posting_list(postings_list, \"postings_gcp_body/\", bucket_name)\n","\n","  # Apply the function to each group of postings and collect the results\n","  posting_locations = postings_grouped.map(write_postings_to_disk)\n","\n","  return posting_locations\n","\n","\n","def calculate_idf2(postings, total_documents):\n","  ''' Takes a posting list RDD, calculates the IDF for each token.\n","  Parameters:\n","  -----------\n","    postings: RDD\n","      An RDD where each element is a (token, posting_list) pair.\n","    total_documents: int\n","      Total number of documents in the corpus.\n","\n","  Returns:\n","  --------\n","    RDD\n","      An RDD where each element is a (token, idf) pair.\n","  '''\n","  # Calculate Document Frequency (DF) for each token\n","  df_rdd = calculate_df(postings)\n","\n","  # Calculate Inverse Document Frequency (IDF) for each token\n","  idf_rdd = df_rdd.map(lambda x: (x[0], log(total_documents / x[1])))\n","\n","  return idf_rdd\n"]},{"cell_type":"code","execution_count":14,"id":"3fb088eb","metadata":{},"outputs":[],"source":["# word counts as flat map\n","word_counts = doc_text_pairs.flatMap(lambda x: word_count(x[0], x[1]))\n","postings_filtered = word_counts.groupByKey().mapValues(reduce_word_counts)\n","\n","# filtering postings and calculate df\n","FILTER_MIN = 25\n","postings_filtered = postings_filtered.filter(lambda x: len(x[1])>FILTER_MIN)\n","\n","w2df = calculate_df(postings_filtered)"]},{"cell_type":"code","execution_count":15,"id":"c22fccf9","metadata":{},"outputs":[],"source":["# כתיבה לבאקט, לא להריץ שוב, עבד!!\n","\n","# w2df_path = \"gs://yuval_206542839/postings_gcp_body/w2df\"\n","\n","# numPartitions = 1000\n","# repartitionedRDD = w2df.repartition(numPartitions)\n","\n","# # Save the repartitioned RDD to GCS\n","# repartitionedRDD.saveAsTextFile(w2df_path)"]},{"cell_type":"code","execution_count":16,"id":"bbadcbaa","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["24/03/09 21:46:25 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"]}],"source":["# Initialize SparkSession\n","spark = SparkSession.builder \\\n","    .appName(\"sparkContext\") \\\n","    .getOrCreate()\n","\n","# Access SparkContext from SparkSession\n","sc = spark.sparkContext\n","\n","# read w2df from bucket\n","path = \"gs://yuval_206542839/postings_gcp_body/w2df\"\n","w2df_dict_str = sc.textFile(path)"]},{"cell_type":"code","execution_count":17,"id":"44f74eec","metadata":{},"outputs":[],"source":["# Function to safely parse the strings to list\n","def safe_parse_to_dict(s):\n","    try:\n","        key, value = eval(s)\n","        return {key: value}\n","    except:\n","        # Return an empty dictionary or None in case of error\n","        return {}"]},{"cell_type":"code","execution_count":18,"id":"b0f554d0","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["# לקח חצי דקה בערך\n","\n","# Parse each string into a tuple and filter out any failures\n","w2df_lst = w2df_dict_str.map(safe_parse_to_dict).collect()"]},{"cell_type":"code","execution_count":19,"id":"380704b4","metadata":{"scrolled":true},"outputs":[],"source":["# create dict from the list\n","w2df_dict = {key: value for d in w2df_lst for key, value in d.items()}"]},{"cell_type":"code","execution_count":20,"id":"5859ed8f","metadata":{},"outputs":[],"source":["# calculate doc_length\n","doc_length = doc_text_pairs.map(lambda row: length_doc(row['text'], row['id']))"]},{"cell_type":"code","execution_count":21,"id":"80f27348","metadata":{},"outputs":[],"source":["# לא להריץ שוב !!!!!\n","\n","# Sum all document lengths\n","# total_length = doc_length.map(lambda x: x[1]).reduce(lambda a, b: a + b)"]},{"cell_type":"code","execution_count":22,"id":"bb79284d","metadata":{},"outputs":[],"source":["# # לא להריץ שוב !!!!!\n","\n","# # Count the number of documents\n","# num_documents = 6348910\n","\n","# # Calculate average length\n","# doc_avg_len = total_length / num_documents if num_documents else 0"]},{"cell_type":"code","execution_count":23,"id":"432287e8","metadata":{},"outputs":[],"source":["# # write to bucket\n","# # לא להריץ שוב !!!\n","\n","# dl_path = \"gs://yuval_206542839/postings_gcp_body/doc_length_dict\"\n","\n","# numPartitions = 1000\n","# repartitionedRDD = doc_length.repartition(numPartitions)\n","\n","# # Save the repartitioned RDD to GCS\n","# repartitionedRDD.saveAsTextFile(dl_path)"]},{"cell_type":"code","execution_count":24,"id":"44f93565","metadata":{},"outputs":[],"source":["# read doc_length_dict from bucket\n","path = \"gs://yuval_206542839/postings_gcp_body/doc_length_dict\"\n","doc_length_str = sc.textFile(path)"]},{"cell_type":"code","execution_count":25,"id":"326ae837","metadata":{"scrolled":true},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["# Parse each string into a tuple and filter out any failures\n","doc_length_lst = doc_length_str.map(safe_parse_to_dict).collect()"]},{"cell_type":"code","execution_count":26,"id":"bb36fa76","metadata":{},"outputs":[],"source":["# dictionary\n","doc_length_dict = {key: value for d in doc_length_lst for key, value in d.items()}"]},{"cell_type":"code","execution_count":27,"id":"6e2cdf0c","metadata":{"scrolled":false},"outputs":[],"source":["# # לא להריץ שוב !!!\n","# # write posting list to bucket\n","# _ = partition_postings_and_write(postings_filtered, \"yuval_206542839\").collect()"]},{"cell_type":"code","execution_count":28,"id":"a7ffe433","metadata":{},"outputs":[],"source":["# collect all posting lists locations into one super-set\n","super_posting_locs = defaultdict(list)\n","for blob in client.list_blobs(bucket_name, prefix='postings_gcp_body'):\n","  if not blob.name.endswith(\"pickle\"):\n","    continue\n","  with blob.open(\"rb\") as f:\n","    posting_locs = pickle.load(f)\n","    for k, v in posting_locs.items():\n","      super_posting_locs[k].extend(v)"]},{"cell_type":"code","execution_count":30,"id":"64b13050","metadata":{},"outputs":[],"source":["# # לא להריץ שוב !!!\n","# # Create inverted index instance\n","# inverted_body = InvertedIndex()\n","\n","# # Adding the posting locations dictionary to the inverted index\n","# inverted_body.posting_locs = super_posting_locs\n","\n","# # Add the token - df dictionary to the inverted index\n","# inverted_body.df = w2df_dict\n","# inverted_body.dict_len = doc_length_dict\n","# inverted_body.N = num_documents\n","# inverted_body.AVG = doc_avg_len\n","\n","# # write the global stats out\n","# inverted_body.write_index('.', 'index_body')\n","\n","# # upload to gs\n","# index_src = \"index_body.pkl\"\n","# index_dst = f'gs://{bucket_name}/postings_gcp_body/{index_src}'\n","# !gsutil cp $index_src $index_dst"]},{"cell_type":"code","execution_count":31,"id":"55c8764e","metadata":{"id":"0b5d7296","nbgrader":{"grade":false,"grade_id":"cell-index_construction","locked":false,"schema_version":3,"solution":true,"task":false},"scrolled":true},"outputs":[{"name":"stdout","output_type":"stream","text":["gs://dataproc-staging-us-central1-860733016545-rvegqajb/\r\n","gs://dataproc-temp-us-central1-860733016545-hd7yfavc/\r\n","gs://yuval_206542839/\r\n"]}],"source":["!gsutil ls -lh $index_dst"]},{"cell_type":"code","execution_count":32,"id":"2e6fb265","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: google-cloud-storage in /opt/conda/miniconda3/lib/python3.10/site-packages (1.43.0)\n","Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/miniconda3/lib/python3.10/site-packages (from google-cloud-storage) (2.28.2)\n","Requirement already satisfied: six in /opt/conda/miniconda3/lib/python3.10/site-packages (from google-cloud-storage) (1.16.0)\n","Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /opt/conda/miniconda3/lib/python3.10/site-packages (from google-cloud-storage) (2.28.1)\n","Requirement already satisfied: google-api-core<3.0dev,>=1.29.0 in /opt/conda/miniconda3/lib/python3.10/site-packages (from google-cloud-storage) (2.17.1)\n","Requirement already satisfied: google-cloud-core<3.0dev,>=1.6.0 in /opt/conda/miniconda3/lib/python3.10/site-packages (from google-cloud-storage) (2.4.1)\n","Requirement already satisfied: google-resumable-media<3.0dev,>=1.3.0 in /opt/conda/miniconda3/lib/python3.10/site-packages (from google-cloud-storage) (2.7.0)\n","Requirement already satisfied: protobuf in /opt/conda/miniconda3/lib/python3.10/site-packages (from google-cloud-storage) (4.21.12)\n","Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /opt/conda/miniconda3/lib/python3.10/site-packages (from google-api-core<3.0dev,>=1.29.0->google-cloud-storage) (1.62.0)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/miniconda3/lib/python3.10/site-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage) (5.3.3)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/miniconda3/lib/python3.10/site-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage) (0.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/miniconda3/lib/python3.10/site-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage) (4.9)\n","Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/miniconda3/lib/python3.10/site-packages (from google-resumable-media<3.0dev,>=1.3.0->google-cloud-storage) (1.1.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/miniconda3/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/miniconda3/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (3.6)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/miniconda3/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/miniconda3/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2024.2.2)\n","Requirement already satisfied: cffi>=1.0.0 in /opt/conda/miniconda3/lib/python3.10/site-packages (from google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=1.3.0->google-cloud-storage) (1.16.0)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/miniconda3/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-cloud-storage) (0.5.1)\n","Requirement already satisfied: pycparser in /opt/conda/miniconda3/lib/python3.10/site-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=1.3.0->google-cloud-storage) (2.21)\n","\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n","\u001B[0mRequirement already satisfied: flask in /opt/conda/miniconda3/lib/python3.10/site-packages (3.0.2)\n","Requirement already satisfied: Werkzeug>=3.0.0 in /opt/conda/miniconda3/lib/python3.10/site-packages (from flask) (3.0.1)\n","Requirement already satisfied: Jinja2>=3.1.2 in /opt/conda/miniconda3/lib/python3.10/site-packages (from flask) (3.1.3)\n","Requirement already satisfied: itsdangerous>=2.1.2 in /opt/conda/miniconda3/lib/python3.10/site-packages (from flask) (2.1.2)\n","Requirement already satisfied: click>=8.1.3 in /opt/conda/miniconda3/lib/python3.10/site-packages (from flask) (8.1.7)\n","Requirement already satisfied: blinker>=1.6.2 in /opt/conda/miniconda3/lib/python3.10/site-packages (from flask) (1.7.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/miniconda3/lib/python3.10/site-packages (from Jinja2>=3.1.2->flask) (2.1.1)\n","\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n","\u001B[0m"]}],"source":["!pip install google-cloud-storage\n","!pip install flask"]},{"cell_type":"code","execution_count":33,"id":"8752aaa4","metadata":{},"outputs":[],"source":["bucket_name = \"yuval_206542839\"\n","index_src = \"index_body\"\n","\n","# import inverted index from the bucket\n","base_dir = \"postings_gcp_body/\"\n","index_body = InvertedIndex.read_index(base_dir, f'{index_src}', bucket_name)"]},{"cell_type":"markdown","id":"7e7a48ff","metadata":{},"source":["SEARCH ENGINE"]},{"cell_type":"code","execution_count":34,"id":"d9b5c594","metadata":{},"outputs":[],"source":["def preprocess_query(query):\n","    \"\"\"\n","    Preprocesses the query by removing stopwords, stemming, and tokenization.\n","    \"\"\"\n","    \n","    # Stopwords list\n","    stop_words = set(stopwords.words('english')) \n","    \n","    # Stemmer\n","    stemmer = PorterStemmer() \n","    \n","    # lower case\n","    query = query.lower() \n","    \n","    # Remove non-alphabetic characters\n","    query = re.sub(r'[^a-zA-Z\\s]', '', query) \n","   \n","    # Tokenize\n","    tokens = query.split() \n","\n","    # Remove stopwords and stem\n","    tokens = [stemmer.stem(word) for word in tokens if word not in stop_words]\n","    \n","    return tokens\n","\n","\n","def query_to_vector(query_tokens, index):\n","    \"\"\"\n","    Transforms a query into a vector using TF-IDF weights from the inverted index.\n","    :param query_tokens: List of tokens in the query\n","    :param index: The inverted index object\n","    :return: A dictionary representing the query vector, with terms as keys and TF-IDF as values.\n","    \"\"\"\n","    # Calculate TF for the query\n","    query_tf = Counter(query_tokens)\n","    query_vector = {}\n","\n","    for term, tf in query_tf.items():\n","        # Calculate TF-IDF, assuming the document contains the term\n","        if term in index.df:\n","            query_vector[term] = tf\n","\n","    return query_vector\n","\n","\n","def cosine_similarity(query_vector,index):\n","    \"\"\"\n","    Calculates the cosine similarity between a query vector and document vectors,\n","    where document vectors are represented by posting lists of tokens that appear both in the documents and the query.\n","    :param query_vector: The query vector represented as a dictionary.\n","    :param posting_lists: A dictionary of posting lists, with terms as keys and lists of (doc_id, tf_idf) as values.\n","    :return: A list of tuples, each tuple being (doc_id, cosine_similarity_score).\n","    \"\"\"\n","    # Initialize document scores and norms\n","    doc_scores = Counter()\n","    doc_norms = Counter()\n","\n","    # Calculate scores for each term in the query\n","    for term, query_weight in query_vector.items():\n","        if term in index.df:\n","            posting = index.read_a_posting_list(\"\",term,\"yuval_206542839\")\n","            for doc_id, tf_idf in posting:\n","                doc_scores[doc_id] += query_weight * tf_idf\n","                doc_norms[doc_id] += tf_idf ** 2\n","\n","    # Calculate query norm\n","    sum_total = 0\n","    for weight in query_vector.values():\n","        sum_total += weight**2\n","    query_norm = math.sqrt(sum_total)\n","\n","    # Calculate final cosine similarity scores\n","    similarities = []\n","    for doc_id, score in doc_scores.items():\n","        doc_norm = math.sqrt(doc_norms[doc_id])\n","        if query_norm == 0 or doc_norm == 0:\n","            similarity = 0\n","        else:\n","            similarity = score / (query_norm * doc_norm)\n","        similarities.append((doc_id, similarity))\n","\n","    return similarities[:1000]\n","\n","\n","def bm25_score(query_vector, index, k1=1.5, b=0.75):\n","    \"\"\"\n","    Calculates the BM25 score between a query vector and document vectors.\n","    :param query_vector: The query vector represented as a dictionary with terms and their TF-IDF weights.\n","    :param index: The inverted index object containing document frequencies and other relevant data.\n","    :param k1: The scaling factor for term frequency. Typically between 1.2 and 2.0.\n","    :param b: The document length normalization factor. Typically close to 0.75.\n","    :return: A list of tuples, each tuple being (doc_id, BM25_score).\n","    \"\"\"\n","    doc_scores = Counter()\n","\n","    for term, query_tf in query_vector.items():\n","        if term in index.df:\n","            posting = index.read_a_posting_list(\"\", term, \"yuval_206542839\")\n","            for doc_id, doc_tf in posting:\n","                # IDF calculation for the term\n","                idf = math.log((index.N - index.df[term] + 0.5) / (index.df[term] + 0.5) + 1)\n","                \n","                # Term frequency normalization and scaling\n","                B = 1 - b + b * (doc_length_dict[doc_id] / index.AVG)\n","                tf_scaled = (doc_tf * (k1 + 1)) / (doc_tf + k1 * B )\n","                # Accumulate the BM25 score for the document\n","                doc_scores[doc_id] += idf * tf_scaled\n","\n","    # Convert scores to a sorted list of tuples (doc_id, score)\n","    sorted_scores = doc_scores.most_common()  # This will sort documents by their BM25 score in descending order\n","    \n","    return sorted_scores[:1000]  # Return top 100 documents, adjust as needed\n","\n","\n","\n","\n","def return_result(page_ids):\n","    \"\"\"\n","    recieve list of page_idf\n","    return list (doc_id,title)\n","    \"\"\"\n","    titles = []\n","    for page_id in page_ids:\n","        url = f'https://en.wikipedia.org/?curid={page_id}'\n","        response = requests.get(url)\n","        if response.status_code == 200:\n","            # Assuming the title is easily extractable from the response content\n","            # This is a simplification; actual extraction may require parsing the HTML or using an API\n","            title_start = response.text.find('<title>') + 7\n","            title_end = response.text.find('</title>', title_start)\n","            title = response.text[title_start:title_end].replace(\" - Wikipedia\", \"\")\n","            titles.append((page_id,title))\n","            \n","    return titles\n","            \n","\n","\n","\n","def my_search(query):\n","    \"\"\"\n","    Performs the search operation, including query preprocessing, document retrieval,\n","    and document ranking.\n","    \"\"\"\n","    index_src_title = \"index\"\n","    index_src_body = \"index_body\"\n","    \n","    base_dir_title = \"postings_gcp/\"\n","    base_dir_body = \"postings_gcp_body/\"\n","    bucket_name = \"yuval_206542839\"\n","    \n","    index_title = InvertedIndex.read_index(base_dir_title, f'{index_src_title}', bucket_name)\n","    index_body = InvertedIndex.read_index(base_dir_body, f'{index_src_body}', bucket_name)\n","    \n","    tokens = preprocess_query(query)\n","    query_vector_title = query_to_vector(tokens, index_title)\n","    query_vector_body = query_to_vector(tokens, index_body)\n","    \n","    sim_cosine = cosine_similarity(query_vector_title, index_title) \n","    sim_bm25 = bm25_score(query_vector_body, index_body, k1=1.5, b=0.75)\n","    \n","    result = calculate_result(sim_bm25,sim_cosine,0.6,0.4)\n","    \n","    result = [doc_id for doc_id, _ in result]\n","\n","    result = result[:100]\n","    \n","    result = return_result(result)\n","    return result\n","\n","\n","\n","def calculate_result(bm25_scores,cosine_similarity_scores,weight_TITLE,weight_BODY):\n","    # Convert lists to dictionaries\n","    \n","    bm25_dict = {key: value for key, value in bm25_scores}\n","    cosine_similarity_dict = {key: value for key, value in cosine_similarity_scores}\n","\n","    # Normalize BM25 scores using min-max normalization\n","    min_bm25 =bm25_scores[-1][1]\n","    max_bm25 =bm25_scores[0][1]\n","    normalized_bm25_dict = {doc_id: (score - min_bm25) / (max_bm25 - min_bm25) for doc_id, score in bm25_dict.items()}\n","\n","    # Initialize a dictionary to store the final weighted scores\n","    final_scores = {}\n","\n","    # Calculate the weighted score for documents appearing in either or both dictionaries\n","    all_doc_ids = set(bm25_dict.keys()).union(set(cosine_similarity_dict.keys()))\n","    for doc_id in all_doc_ids:\n","        bm25_score = normalized_bm25_dict.get(doc_id, 0)\n","        cosine_score = cosine_similarity_dict.get(doc_id, 0)\n","        final_scores[doc_id] = weight_BODY * bm25_score + weight_TITLE * cosine_score\n","\n","    # Optional: Sort the final scores dictionary by score in descending order to see the highest ranked documents first\n","    sorted_final_scores = sorted(final_scores.items(), key=lambda x: x[1], reverse=True)\n","\n","    # Display the sorted final scores\n","    return sorted_final_scores\n"]},{"cell_type":"code","execution_count":35,"id":"5c7db520","metadata":{"scrolled":true},"outputs":[{"data":{"text/plain":["[(65476855, 'Hello FM (Ghana)'),\n"," (28719199, 'Hello Hey'),\n"," (6644261, 'Hello Kitty: Roller Rescue'),\n"," (59612068, 'Hello Molly'),\n"," (13834, '\"Hello, World!\" program'),\n"," (34043293, 'Hello Dummy!'),\n"," (49272424, 'Hello Gorgeous'),\n"," (7956284, 'Hello (Aya Ueto song)'),\n"," (28063614, 'List of Hello Kitty animated series'),\n"," (12409428, 'Say Hello, Wave Goodbye'),\n"," (33361179, 'Hello! Canada'),\n"," (61882884, 'Hello Youmzain'),\n"," (3162118, 'Hello, Larry'),\n"," (1480256, 'Hello! Morning'),\n"," (1895881, 'Hello, Dolly! (song)'),\n"," (7375587, 'Hello Tomorrow'),\n"," (40434533, 'The Oh Hellos'),\n"," (46740416, 'The Oh, Hello Show'),\n"," (1379058, 'Hello Muddah, Hello Fadduh (A Letter from Camp)'),\n"," (1206479, 'Hello, Goodbye'),\n"," (3748733, 'Hello Sailor'),\n"," (7531070, 'Hello Kitty no Hanabatake'),\n"," (3333421, 'Hello (disambiguation)'),\n"," (16393064, 'The Adventures of Hello Kitty &amp; Friends'),\n"," (3047381, 'Hello Mary Lou'),\n"," (42663426, 'Hello Hello'),\n"," (11896394, 'Hello Sunshine (Super Furry Animals song)'),\n"," (40778942, 'Hello Kitty (disambiguation)'),\n"," (37802793, 'Hello World'),\n"," (7987599, 'Hello Pop!'),\n"," (20220062, 'Hello Goodbye (disambiguation)'),\n"," (29290561, 'Hello and Goodbye'),\n"," (47972617, 'Hello There, Universe'),\n"," (43125964, 'Hello Twelve, Hello Thirteen, Hello Love'),\n"," (5991011, 'Hello (Poe song)'),\n"," (54295, 'Hello Kitty'),\n"," (54077698, 'Hello Neighbor'),\n"," (1241470, 'Hello! Project'),\n"," (45403422, 'Hello Muddah, Hello Faddah!'),\n"," (36776451, 'Hello My Name Is'),\n"," (4914946, 'Hello Sailor'),\n"," (13544634, \"Hello Darlin' (song)\"),\n"," (29972030, 'Hello (Martin Solveig and Dragonette song)'),\n"," (3587577, 'Hello Again'),\n"," (54283448, 'Hello English'),\n"," (59100671, 'Hello My Love'),\n"," (35988134, 'Hello Stranger'),\n"," (4877259, \"Hello, Hello, I'm Back Again\"),\n"," (6704430, 'Hello Young Lovers'),\n"," (1895130, 'Hello, Dolly!'),\n"," (29915530, 'Hello Love'),\n"," (24452551, 'Hello! (Good to Be Back)'),\n"," (18569719, 'Hello Heartbreak'),\n"," (20096694, 'Hello Kitty: Big City Dreams'),\n"," (32504178, 'GNU Hello'),\n"," (39018794, 'Hello Darling (disambiguation)'),\n"," (6435365, 'Hello Saferide'),\n"," (7336487, 'Hello America (disambiguation)'),\n"," (36067552, 'Hello Cruel World'),\n"," (244825, 'Hello I Must Be Going'),\n"," (44442131, 'Goodbye and Hello'),\n"," (1977205, 'World Hello Day'),\n"," (8845033, \"Hello Darlin'\"),\n"," (6237239, 'Hello I Love You'),\n"," (32693099, 'Hello Beautiful (disambiguation)'),\n"," (67804910, 'Hello Sunshine'),\n"," (26961074, 'You Had Me at Hello'),\n"," (6710844, 'Hello'),\n"," (15702996, 'Hello Afrika (song)'),\n"," (11643222, 'Golden hello'),\n"," (10111613, 'Hello (Turn Your Radio On)'),\n"," (2677658, 'Hello CD of the Month Club'),\n"," (63092728, 'Say Hello (Breathe song)'),\n"," (44284425, 'Hello Franceska'),\n"," (15867959, 'Hello Brother'),\n"," (16828332, 'Hello Again (The Cars song)'),\n"," (53330916, 'Hello Curry'),\n"," (55931873, 'Hello Summer'),\n"," (51264645, 'Hello (social network)'),\n"," (9757384, 'Hello Cleveland'),\n"," (42450420, 'Hello bank!'),\n"," (27974555, 'I Just Dropped by to Say Hello'),\n"," (43677470, 'Hello Good Morning (disambiguation)'),\n"," (33107024, 'Hello Motherfucker!'),\n"," (23421892, 'Hello Stranger (song)'),\n"," (59158681, 'Hello (company)'),\n"," (15471050, 'Say Hello 2 Heaven'),\n"," (12126911, \"Hello Kitty's Cube Frenzy\"),\n"," (23378146, 'Douglas v Hello! Ltd'),\n"," (54964875, 'Hello (2017 film)'),\n"," (40049488, 'Hello (Kelly Clarkson song)'),\n"," (33942076, 'Hello (The Potbelleez song)'),\n"," (34580660, 'Hello World! (composition)'),\n"," (52078248, 'Hello (Hedley song)'),\n"," (12559739, 'Hello (The Cat Empire song)'),\n"," (35929520, 'Hello Muddah, Hello Faddah! (book)'),\n"," (54503643, 'Hello Sunshine (company)'),\n"," (6977878, 'Say hello to my little friend'),\n"," (619375, 'Hello (The Beloved song)'),\n"," (49556681, 'Hello Friday')]"]},"execution_count":35,"metadata":{},"output_type":"execute_result"}],"source":["q = \"hello, running\"\n","my_search(q)"]}],"metadata":{"celltoolbar":"Create Assignment","colab":{"collapsed_sections":[],"name":"assignment3_gcp.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"PySpark","language":"python","name":"pyspark"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"}},"nbformat":4,"nbformat_minor":5}